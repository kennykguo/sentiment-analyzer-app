{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8779df-3885-4611-913f-8fbbf34e0752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (69.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/kenny/Library/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08385d50-8fd0-4016-9933-5eb80e5f69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x13255b3b0>\n",
      "\n",
      "Dave watched as the forest burned up on the hill,\n",
      "only a few miles from his house. The car had\n",
      "been hastily packed and Marta was inside trying to round\n",
      "up the last of the pets. \"Where could she be?\" he wondered\n",
      "as he continued to wait for Marta to appear with the pets.\n",
      "\n",
      "[\n",
      ", Dave, watched, as, the, forest, burned, up, on, the, hill, ,, \n",
      ", only, a, few, miles, from, his, house, ., The, car, had, \n",
      ", been, hastily, packed, and, Marta, was, inside, trying, to, round, \n",
      ", up, the, last, of, the, pets, ., \", Where, could, she, be, ?, \", he, wondered, \n",
      ", as, he, continued, to, wait, for, Marta, to, appear, with, the, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Testing spaCy\n",
    "\n",
    "text = \"\"\"\n",
    "\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# NLP constructor\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(doc)\n",
    "\n",
    "token_list = [token for token in doc]\n",
    "print(token_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b68804-d43a-4a27-a1dd-88b6429a0a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, forest, burned, hill, ,, \n",
      ", miles, house, ., car, \n",
      ", hastily, packed, Marta, inside, trying, round, \n",
      ", pets, ., \", ?, \", wondered, \n",
      ", continued, wait, Marta, appear, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d451e4ac-8c45-408a-9c30-fb9e8e7456b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (580949051.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[129], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Stemming and lemmatization (only lemmatization is provided by spaCy)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Normalize words\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Stemming and lemmatization (only lemmatization is provided by spaCy)\n",
    "Note the underscore returns the readable version of the lemma here\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lemmas = [\n",
    "    f\"Token: {token}, lemma: {token.lemma_}\"\n",
    "    for token in filtered_tokens\n",
    "]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541de5d0-f7c6-45d3-81b0-f17dbf2cc75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0732636 , -1.5893133 , -0.7485422 ,  0.80338854,  0.199772  ,\n",
       "        0.00840409,  1.5419112 ,  0.78789294, -0.10507858, -0.08379468,\n",
       "        1.6370184 ,  0.9981552 , -0.27276078, -0.90784246, -1.248598  ,\n",
       "       -0.5253062 ,  0.36606142,  0.3220521 ,  0.26947665, -0.6838576 ,\n",
       "       -1.3466266 ,  0.01122165, -0.24088567, -0.48466757, -0.33174923,\n",
       "       -0.05325297,  1.8773435 ,  0.5649502 , -0.9605744 ,  0.78610945,\n",
       "       -0.44939822, -1.4648836 , -0.38066614,  1.0480766 , -0.83412176,\n",
       "       -0.2217491 , -0.854434  ,  0.35594553, -0.11274697,  1.2787786 ,\n",
       "       -0.8223142 ,  0.18473107, -0.08983883,  0.6325264 , -1.1029457 ,\n",
       "        0.37194866,  0.11167954,  1.529881  ,  0.73127055, -0.01238401,\n",
       "       -0.38741043,  0.24374214,  0.66934144, -0.5147386 , -0.05107623,\n",
       "       -0.68364084,  1.2553529 , -0.4258138 ,  0.8257121 , -0.40289953,\n",
       "       -1.0714421 ,  0.8215431 ,  0.1035445 , -0.5627636 ,  0.34108096,\n",
       "       -0.46954772, -0.6444609 , -0.4248718 , -0.74732184, -0.93381315,\n",
       "        0.54961896,  0.95218015,  1.8080062 ,  0.77191067, -0.06320465,\n",
       "       -1.1639178 , -0.37379634, -2.0506837 , -0.2422083 , -0.6951567 ,\n",
       "       -0.68725175,  0.7614975 , -0.8780632 ,  0.6869065 ,  1.4673206 ,\n",
       "       -0.3449759 ,  0.805764  , -0.30646116, -0.6471175 ,  0.15497795,\n",
       "       -0.39263946, -0.12381144,  2.092909  ,  0.07248274,  1.3887855 ,\n",
       "        1.6653665 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the text\n",
    "filtered_tokens[1].vector\n",
    "# Dense arrays have defined values in every space of the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc8964f-877a-4947-9376-fe0edd292197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "# 80% training data, 20% test data\n",
    "\n",
    "\"\"\"\n",
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read().strip()\n",
    "                label_id = 1 if label == \"pos\" else 0\n",
    "                reviews.append((text, {\"cats\": {\"POSITIVE\": label_id, \"NEGATIVE\": 1 - label_id}}))\n",
    "    random.shuffle(reviews)\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]\n",
    "\"\"\"\n",
    "\n",
    "def load_training_data(data_directory: str = \"aclImdb/train\", split: float = 0.8, limit: int = 0) -> tuple:\n",
    "    # Create a list to store the list of tuples\n",
    "    reviews = [] # Create a list to store the reviews\n",
    "    # Create two directory paths\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        # Create the full directory path to the pos and neg folders\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        print(labeled_directory)\n",
    "        # Create the full directory to each of the text files in the pos and negative folders\n",
    "        # Iterating over all files in the pos and neg folders\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read().strip()\n",
    "                label_id = 1 if label == \"pos\" else 0\n",
    "                reviews.append((text, {\"cats\": {\"POSITIVE\": label_id, \"NEGATIVE\": 1 - label_id}}))\n",
    "    \n",
    "    # We created a list of tuples. If we denote a tuple as example, then example[0] is the text, and example[1] is the sentiment\n",
    "    random.shuffle(reviews)\n",
    "    # If a limit was passed in, limit the total number of reviews\n",
    "    # If 0 was passed in, this means there is no limit and this code does not run\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    # First is the training set, second is the validation/test set\n",
    "    return reviews[:split], reviews[split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41725697-0ed4-431e-a97d-b5428680cac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"This comedy is bound to be good from the get-go. East meets west and east doesn't want to lose...west doesn't know what losing is like. It starts a little slow but it grabs you very soon and it doesn't let go. This is definitely worth seeing.\", {'cats': {'POSITIVE': 1, 'NEGATIVE': 0}})]\n"
     ]
    }
   ],
   "source": [
    "# One data entry looks like:\n",
    "print (train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b536ea28-75fd-41fd-94f7-fc010cd929c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def train_model(train_data: list, test_data: list, iterations: int = 5) -> None:\n",
    "    # Initalize the spaCy constructor\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    # Ensures that if we update the code, we get the correct \"textcat\"\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "    textcat.add_label(\"POSITIVE\")\n",
    "    textcat.add_label(\"NEGATIVE\")\n",
    "\n",
    "    # Create a listz of excluded pipes. We aren't actually using any other pipes. This is just a double check\n",
    "    training_excluded_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "\n",
    "    # Disable all pipeline components not being used\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Start of training:\")\n",
    "        print(\"Loss\\t\\tPrecision\\t\\tRecall\\t\\tF-score\")\n",
    "        # Starting batch size, maximum batch size, rate at which batch size grows in each iteration\n",
    "        # This allows the model to learn from a variety of batch sizes, potentially increasing accuracy\n",
    "        batch_sizes = compounding(4, 32.0, 1.001)\n",
    "        for i in range(iterations):\n",
    "            loss = {}\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for text, label in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    examples.append(Example.from_dict(doc, label))\n",
    "                nlp.update(examples, losses=loss)\n",
    "                \n",
    "            # Evaluate the model\n",
    "            evaluation_results = evaluate_model(nlp.tokenizer, textcat, test_data)\n",
    "            print(\n",
    "                f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                f\"\\t{evaluation_results['recall']}\\t{evaluation_results['f-score']}\"\n",
    "            )\n",
    "    # Save the model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dfbb329-ab21-47dd-876e-2dc622d89c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the progress of training\n",
    "# True positive, true negative, false positive, false negative\n",
    "# Calculate the precision (# of successful classification vs # of incorrect)\n",
    "# Calculate the recall (# of sucessful classification / # of incorrect)\n",
    "# F-score - metric for evaluating effectiveness of a binary classification model\n",
    "\n",
    "def evaluate_model(tokenizer, textcat, test_data: list) -> dict:\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for text, annotation in test_data:\n",
    "        doc = tokenizer(text)\n",
    "        truth = annotation[\"cats\"]  # True labels\n",
    "        predicted = textcat(doc)    # Predicted labels and scores\n",
    "        \n",
    "        for label, score in predicted.cats.items():\n",
    "            if label not in truth:\n",
    "                continue  # Skip if the label is not in the true labels\n",
    "            \n",
    "            # Check if the predicted score meets a threshold (e.g., 0.5) to classify as positive or negative\n",
    "            if score >= 0.5:\n",
    "                # Predicted as positive\n",
    "                if truth[label] == 1:\n",
    "                    # True positive\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    # False positive\n",
    "                    false_positives += 1\n",
    "            else:\n",
    "                # Predicted as negative\n",
    "                if truth[label] == 0:\n",
    "                    # True negative\n",
    "                    true_negatives += 1\n",
    "                else:\n",
    "                    # False negative\n",
    "                    false_negatives += 1\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f92c9b2-31bb-4a63-b95f-15143857c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb/train/pos\n",
      "aclImdb/train/neg\n"
     ]
    }
   ],
   "source": [
    "# train and test, are lists of tuples\n",
    "# The limit parameter limits the number of training and testing examples\n",
    "train, test = load_training_data(limit=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7fcdcb78-a27f-4c58-8b15-0c17a107daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of training:\n",
      "Loss\t\tPrecision\t\tRecall\t\tF-score\n",
      "91.11163692548871\t0.78199999998436\t0.78199999998436\t0.7819999999843599\n",
      "27.476687906397274\t0.7599999999848\t0.7599999999848\t0.7599999999848\n",
      "7.321709191841013\t0.76799999998464\t0.76799999998464\t0.76799999998464\n",
      "1.2663690280867284\t0.80399999998392\t0.80399999998392\t0.8039999999839199\n",
      "0.05400360797214075\t0.79599999998408\t0.79599999998408\t0.79599999998408\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c627f09-fa02-4047-abdc-c669251670fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category probabilities:\n",
      "{'POSITIVE': 0.8926675915718079, 'NEGATIVE': 0.10733240097761154}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "nlp = spacy.load(\"model_artifacts\")\n",
    "\n",
    "from string import punctuation\n",
    "#load spacy's default language\n",
    "\n",
    "# Test review\n",
    "TEST_REVIEW = \"\"\"\n",
    "Friendly and efficient customer service. \n",
    "The store owner was really patient. \n",
    "My new glasses arrived in less than a week and they \n",
    "made sure every details and aspects work perfectly for me. \n",
    "Overall, I'm really satisfied with it.\n",
    "This store also gives reasonable price compared to others. \n",
    "I'd recommend trying this store out!\n",
    "\"\"\"\n",
    "\n",
    "# Process the test review\n",
    "doc = nlp(TEST_REVIEW)\n",
    "\n",
    "# Get the predicted category\n",
    "predicted_category = doc.cats\n",
    "print(\"Predicted category probabilities:\")\n",
    "print(predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c24e4be-8e54-491a-bc94-734f1148038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(customer, 1), (service, 1), (store, 1), (owner, 1), (glasses, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Keyword extraction\n",
    "# Using Counter, extract the top five nouns from all reviews combined\n",
    "# Here, we are only using one review\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(TEST_REVIEW)\n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "# Define functions\n",
    "def is_token_allowed(token):\n",
    "    # Filter tokens that are NOT punctuation or stop words\n",
    "    return bool(token and str(token).strip() and not token.is_stop and not token.is_punct)\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Lowercase, and remove whitespace\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "complete_filtered_tokens = Counter([preprocess_token(token) for token in doc if is_token_allowed(token)])\n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adjectives.append(token)\n",
    "\n",
    "\n",
    "top_nouns = Counter(nouns).most_common(5)\n",
    "\n",
    "print(top_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d2e22-30ea-453a-b6d5-b06a00ddcb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
